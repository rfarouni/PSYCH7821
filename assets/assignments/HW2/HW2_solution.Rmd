---
title: "HW2 Solution"
author: "Rick Farouni"
date: "September 20, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## Step 1: Load the data

```{r }
Q1_df <- read.table("~/Documents/repos/PSYCH7821/assets/assignments/HW2/ex02_question1_data.txt", 
                    header = TRUE)

```


## Step 2: Inspect the data

```{r }
head(Q1_df)
```

We see that the variables *sex* and *sctyp* are categorical variables; *civics* and *motiv* need more inspection. Let's compute the contingency table of the counts:

```{r }
table(Q1_df$motiv)
table(Q1_df$civics)
```

Now we can see that *motiv* is a an ordinal variable, more specifically it is a polytomous variable with four levels. *civics* on the other hand seems to have a mixture distribution. The data has obvioulsly been previoulsy preprocessed in a way that might affect the analysis. 


> NOTE 1: The optimal coefficient $\beta$ the regression fit gives us back depends on the distributions of the predictor variables!

Since we have standardized the data, let's compute the correlation matrix. Note that I am leaving out *civics* and *motiv* since correlation is a measure of linear assoication and these two variables are not really continous. 

```{r }

cor(Q1_df[ , 6:11])

```

If we plan to fit a regression model, when need to worry about collinearity. The 4 test ability variables seem correlated among each other, so we need to deal with collinearity. One solution is to obtain a second dataset with uncorrelated predictor variables.  A second approach is regularization, but that is more approporiate for  high-dimensional datasets (n<<p). A third approach is to identify the correlated variables and just consider a single one to include in the model. Since this approach is not so straightforward, principal components analysis can be used instead to reduce the  correlated variables into one or two independent variables.


## Step 3: Plot the data


This the most important step in data analysis. If you do not plot your data first, you are just stumbling in the dark. The data we have is multidimensional, so a pairs plot is a good way to try to see any patterns in the data. You can use the *pairs* function to plot the data, but I am writing my own function so I can use web graphics instead. 

```{r }
library(rbokeh)
plotPairs <- function(df, colms, colr){
  
nms <- expand.grid(names(df)[colms],
                   rev(names(df)[colms]), 
                   stringsAsFactors = FALSE)
splom_list <- vector("list", length(nms))
for (ii in seq_len(nrow(nms))) {
  splom_list[[ii]] <- figure(width = 120, 
                             height = 120, 
                             tools = c("pan", "box_zoom", "reset"), 
                             xlab = nms$Var1[ii],
                             ylab = nms$Var2[ii]) %>%
                      ly_points(nms$Var1[ii], 
                                nms$Var2[ii], 
                                data = df,
                                color = as.factor(df[ , colr]),
                                size = 5, 
                                legend = FALSE)
}

p <- grid_plot(splom_list, 
          ncol = length(colms), 
          same_axes = TRUE, 
          link_data = TRUE)

return(p)
}

```


Let's plot columns 4 to 11 and color the observations by the sex (Variable 2)

```{r }
plotPairs(Q1_df, 4:11, 2)

```

Not all the variables are on the same scale. That is problematic if we are running PCA. Note: *read*, *write*, *math*, and *science* seem to be measured on the same scale so there is no need to standardize the variable for PCA. That said, since we are analyzing the entire dataset which contain variables at differnt scales, it makes more sense to standardize the entire dataset (except of course the categorical variable). 

Next we standardize and plot the data.

```{r }
Q1_df[ , 4:11] <- scale(Q1_df[ , 4:11],
                              center = TRUE, 
                              scale = TRUE)
plotPairs(Q1_df, 4:11, 2)
```


## Step 4: Fit model


### Dimensionality Reduction
  
  
Let's reduce the 4 dimensional data into fewer dimensions. You can fit the model with a single call to the function prcomp like this

```{r }

pca_fit <- prcomp(~read + write + math + science, 
                  data = Q1_df,
                  center = TRUE,
                  scale = FALSE)
```

Scale is FALSE since the data has been standardized already. The proportion of variance explained can be obtained using the summary function
```{r }

summary(pca_fit)

```

Now pca_fit is a list with three objects (rotation, sdev, X). You can access the objects with the $ operator, so for example:

## Question 1A

Q is the loading matrix (eigenvectors)

```{r}
Q <- pca_fit$rotation 
Q
```


Z is the matrix of sample PC scores. sigma is the vector of standard deviations

```{r}
X <- scale(Q1_df[ , 8:11], 
           center = TRUE, 
           scale = FALSE)
Z <- X %*% Q 
# or equivalently
Z <-  pca_fit$x 
# standardized
sigma <- pca_fit$sdev 
Zs <- Z %*% diag(1/sigma)

```

The first standardized first principle component can be obtained by subsetting
```{r}
z1s <- Zs[ , 1]
```

## Question 1B

Lets add PC1 to the dataset
```{r}
Q1_df <- cbind(Q1_df,z1s)
```

### plot the data

```{r}
plotPairs(Q1_df, c(4:7,12), 2)
```

The predictor variables don't seem to be correlated.

### Now we can fit the model

```{r}
fit1 <- lm(z1s ~ sex + sctyp + civics + motiv + locus + concept, 
           data = Q1_df)
summary(fit1)
```


## Step 5: Dignostics

```{r}
# plot diagnostics
par(mfrow = c(2,2))
plot(fit1)
```

Models assumptions seem to hold for the most part. More about how to interpret these plots [here](http://data.library.virginia.edu/diagnostic-plots/). 

## Step 6: Model Selection

To find which of these 6 variables are the most important and statistically significant predictors, we can perform stepwise regression using the AIC. 

> NOTE 2: The step function in R takes proper account of the number of parameters fit ,dropping or adding variables (sometimes in a group) that minimizes the AIC score. If the package you are using does selection on F-statistics instead, by adding “significant” terms  and dropping “non-significant” terms, then multiple comparison issues are not properly dealt with by your package (Friedman, J., Hastie, T., & Tibshirani, R., 2001) and I would recommond switching to another package.  

```{r}
stepFit <- step(fit1)
```

The final model is

```{r}
summary(stepFit)
```


# Question 2 

## Step 1: Load the data
```{r }
Q2_df <- read.table("~/Documents/repos/PSYCH7821/assets/assignments/HW2/ex02_question2_data.txt", 
                    header = TRUE)
```


## Step 2: Inspect the data

```{r }
head(Q2_df)
summary(Q2_df)
```

Except for *gpa*, the other variables seem to gave been measured on the same reference-normed scale. Accordingly, there doesn't seem to be a need for standardization. For PCA we also keep the optional scaling off (the default).

## Step 3: Plot the data

```{r}
plotPairs(Q2_df, 2:7, 1)
```

The 6 variables all seem to be highly correlated. 

## Step 4: fit the data

### Question 2a 

```{r }
fit2 <- lm(gpa ~ spatial2d + spatial3d + mech_res + arith + computat, 
           data = Q2_df)
summary(fit2)
```

> NOTE 3: We see that only *computat* shows statistical significance, but that is expected given that the variables are correlated. Even if we have a situation where each of two predictors perfectly predicts the reponse, youwill see that only one of the two will have a highly significant p-value when two are highly correlated. We need to deal with multicollinearity first.

### Question 2b (Dimensionality Reduction)


First we run PCA on the 5 variables

```{r }
pca_fit2 <- prcomp(~spatial2d + spatial3d + mech_res + arith + computat, 
                  data = Q2_df,
                  center = TRUE,
                  scale = FALSE)
```

The proportion of variance explained can be obtained using the summary function
```{r }

summary(pca_fit2)

```

Now pca_fit is a list with three objects (rotation, sdev, X). You can access the objects with the $ operator, so for example:

Q is the loading matrix (eigenvectors)

```{r}
Q <- pca_fit2$rotation 
Q
```
Z is the matrix of sample PC scores.
```{r}
Z <-  pca_fit2$x
```

### Question 2c 

First we add the five principle components to the dataframe. Next, we fit the model with all five PCs
```{r}
Q2_df <- cbind(Q2_df, Z)
fit3 <- lm(gpa ~ PC1 + PC2 + PC3 + PC4 + PC5, 
           data = Q2_df)
summary(fit3)
```


#### Dignostics

```{r}
# plot diagnostics
par(mfrow = c(2, 2))
plot(fit3)
```

Nothing alarming! we move on the model selection

```{r}
stepFit3 <- step(fit3)
```

The final model is

```{r}
summary(stepFit3)
```


We see that the fifth principle component was included in the model. What to do? For a more interpertable model, just keep the first two.


> NOTE 4: Note that I haven't mentioned R-squared  here at all. The reason is people have begining to realize that R-squared is a useless criterion for model fit.

The reason for that are as follows:

1. R-squared does not measure goodness of fit.
2. R-squared does not measure predictive error.
3. R-squared does not allow you to compare models using transformed responses.
4. R-squared does not measure how one variable explains another.

These reasons are explained really well [here](http://data.library.virginia.edu/is-r-squared-useless/)


==========================================




