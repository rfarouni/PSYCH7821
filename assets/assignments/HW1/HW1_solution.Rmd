
---
title: 'Homework 1: Matrix Algebra'
author: "Rick Farouni"
date: "August 22, 2016"
output:
  html_document:
    toc: yes
---

```{r echo=FALSE}
options(digits = 2)
```

# Introduction
This section shows how to perform some matrix operations in R.

### Creating a Matrix

In R, the basic data structure is a vector. A matrix is basically a vector with a dimension attribute, dim() .
To create a matrix, we use the matrix() function. This function has the following arguments:

```{r}
args(matrix)
# fill matrix by column (default)
```
Here we populate the matrix with a vector
```{r}
v <- c(1, 2, 3, 4, 5, 6)
v
M <-  matrix(v , nrow = 2, ncol = 3)
M
```

### Matrix operations

1. Matrix transpose $\mathbf{M}^T$

```{r}
Mt <-  t(M)
Mt
```

2. Matrix multiplication $\mathbf{M} \mathbf{M}^T$


```{r}
MMt <-  M %*% Mt
MMt
```

3. Matrix addition $\mathbf{M} + \mathbf{M}$

```{r}
dM <-  M + M
dM
```

4. Matrix inverse ${(\mathbf{M} \mathbf{M}^T)}^{-1}$

```{r}
MMt_inverse <-  solve(MMt)
MMt_inverse
```

# Questions

Consider the simple regression model 

$$y_i = \beta_0 + \beta_1 x_i +\epsilon_i  \textrm{    for  } i= 1, 2, \ldots ,n $$ 
In matrix form, the regression model can be represented as

$$\begin{bmatrix}
y_{1} \\ 
\vdots\\ 
y_{n}
\end{bmatrix} =
\begin{bmatrix} 
1 &  x_{1} \\
\vdots\\
1 & x_{n}
\end{bmatrix}
\begin{bmatrix} 
\beta_{0} \\ 
\beta_{1}
\end{bmatrix}
+
\begin{bmatrix} 
\epsilon_{1} \\ 
\vdots\\ 
\epsilon_{n}
\end{bmatrix}$$
or more concisely as 

$$\mathbf{y} =\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

It can be shown that the estimated parameters are

$$\hat{\boldsymbol \beta} = \left( \mathbf{X}^{\mathsf{T}} \mathbf{X} \right)^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y}$$

and the fitted values are

$$\hat{\mathbf{y}} = \mathbf{X} \hat{\boldsymbol \beta} = \mathbf{X} \left( \mathbf{X}^{\mathsf{T}} \mathbf{X} \right)^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y}$$

The next three question involves working with the following data:


```{r}
x <- c(10, 8, 13, 9, 11, 14, 6 , 4, 12, 7 ,5 )
y <-  c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68 )
```

## Question 1: 

Compute the vector of estimated parameters $\hat{\boldsymbol \beta}$ for the simple regression problem given the data provided above.

#### Solution: 

First we need to the column vector of ones. The cbind function binds the two vectors into a matrix. 
```{r}
X <- cbind(1, x)
Xt <- t(X)
beta_hat <- solve(Xt %*% X) %*% Xt %*% y
beta_hat
```

Let's compare our solution with the built-in linear model fit function. The results are identical.

```{r}
model_fit <- lm.fit(X, y)
cbind(beta_hat, lmfit = model_fit$coefficients)
```

## Question 2:

Obtain the fitted values, the vector $\hat{\mathbf{y}}$, by using matrix multiplication

#### Solution: 

```{r}
y_hat <- X %*% beta_hat 
y_hat 
```

let's compare
```{r}
cbind(y_hat, lmfit = model_fit$fitted.values)
```

## Question 3: 

Obtain the vector of residuals $\hat{\boldsymbol{\epsilon}}$. Note that the vector of residuals has a hat on it, whereas the vector of errors $\boldsymbol{\epsilon}$ doesn't. Now you know the difference!

#### Solution: 


```{r}
epsilon_hat <- y - y_hat
epsilon_hat
```

let's compare
```{r}
cbind(epsilon_hat, lmfit = model_fit$residuals)
```

Note: MSE (mean squared error) is just 

```{r}
mean(epsilon_hat ^ 2)
```


## Question 4: 

When you ever see a matrix, think *transformation*! In the equation for obtaining the fitted values above, we see that $\mathbf{y}$ gets multiplied (transformed) by several matrices to give us $\hat{\mathbf{y}}$. Let's denote the result of these multiplications by  
$$\mathbf{H}=\mathbf{X} \left( \mathbf{X}^{\mathsf{T}} \mathbf{X} \right)^{-1} \mathbf{X}^{\mathsf{T}}$$

Obtain $\mathbf{H}$ for the data given above.

#### Solution: 


```{r}
H <- X %*% solve(Xt %*% X) %*% Xt
H
```

Now play with the interactive visualization [here](http://setosa.io/ev/ordinary-least-squares-regression/) and try to figure out what kind of tranformation $\hat{\mathbf{H}}$ accomplished. The answer is one word only. Think about the dimension of $\mathbf{H}$ in relation with the dimension of the data vector $\mathbf{y}$  

#### Solution: 

H is a projection matrix with dimensions 11 by 11, which is also the number of observations we have. The matrix projects the data into points residing on a two-dimensional plane "determined" (spanned by the columns) of $\mathbf{X}$


By the  way, the data is taken from [Anscombe's_quartet](https://www.wikiwand.com/en/Anscombe's_quartet).



